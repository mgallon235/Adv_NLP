{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ufxzo5GdOaz"
      },
      "source": [
        "# Sentiment Analysis on Rotten Tomatoes Dataset II - Homework 2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing / seb / joaquin / mikel"
      ],
      "metadata": {
        "id": "mE-TAkekdYvb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LM_fnTuCeg8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l06WFgIIOp6P"
      },
      "source": [
        "## Metrics Functions to Consider\n",
        "\n",
        "1. **Accuracy**: Measures the proportion of correct predictions among the total number of cases examined. It's a straightforward metric but can be misleading if the classes are imbalanced.\n",
        "\n",
        "2. **Precision and Recall**: Precision measures the proportion of positive identifications that were actually correct, while recall measures the proportion of actual positives that were identified correctly. These metrics are especially important when dealing with imbalanced datasets.\n",
        "\n",
        "3. **F1 Score**: The harmonic mean of precision and recall. It's a good way to show that a classifer has a good balance between precision and recall.\n",
        "\n",
        "4. **Confusion Matrix**: A table used to describe the performance of a classification model on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm.\n",
        "\n",
        "5. **ROC and AUC**: The receiver operating characteristic curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system. The area under the curve (AUC) represents measure of separability."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)\n",
        "import transformers\n",
        "print(transformers.__version__)\n",
        "import datasets\n",
        "print(datasets.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llf-cbGYFOn8",
        "outputId": "96a4e80f-6cd3-4817-a37e-60ce7a52ac0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n",
            "4.37.2\n",
            "2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#nltk.download('stopwords')\n",
        "#!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lOK6TKvfqCI",
        "outputId": "c3092880-70e6-49f5-fc24-d7004cf56360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkfXFXf2Oruj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bbdf1e27-833e-41fa-ec57-9e125c1fc990"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7428fb8c222f>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mlmtzr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Preprocessing\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "import spacy\n",
        "import pickle\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "\n",
        "\n",
        "#getting a library of stopwords and defining a lemmatizer\n",
        "porter=SnowballStemmer(\"english\")\n",
        "lmtzr = WordNetLemmatizer()\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "class Metrics:\n",
        "    def __init__(self):\n",
        "        self.results = {}\n",
        "\n",
        "    def run(self, y_true, y_pred, method_name, average='macro'):\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision = precision_score(y_true, y_pred, average=average)\n",
        "        recall = recall_score(y_true, y_pred, average=average)\n",
        "        f1 = f1_score(y_true, y_pred, average=average)\n",
        "\n",
        "        # Store results\n",
        "        self.results[method_name] = {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "        }\n",
        "\n",
        "    def plot(self):\n",
        "        # Create subplots\n",
        "        fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "        # Plot each metric\n",
        "        for i, metric in enumerate(['accuracy', 'precision', 'recall', 'f1']):\n",
        "            ax = axs[i//2, i%2]\n",
        "            values = [res[metric] * 100 for res in self.results.values()]\n",
        "            ax.bar(self.results.keys(), values)\n",
        "            ax.set_title(metric)\n",
        "            ax.set_ylim(0, 100)\n",
        "\n",
        "            # Add values on the bars\n",
        "            for j, v in enumerate(values):\n",
        "                ax.text(j, v + 0.02, f\"{v:.2f}\", ha='center', va='bottom')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "######### Additional Functions\n",
        "\n",
        "#Returns words without any special\n",
        "def strip(word):\n",
        "    mod_string = re.sub(r'\\W+', '', word)\n",
        "    return mod_string\n",
        "\n",
        "#the following leaves in place two or more capital letters in a row\n",
        "#will be ignored when using standard stemming\n",
        "def abbr_or_lower(word):\n",
        "    if re.match('([A-Z]+[a-z]*){2,}', word):\n",
        "        return word\n",
        "    else:\n",
        "        return word.lower()\n",
        "\n",
        "#modular pipeline for stemming, lemmatizing and lowercasing\n",
        "#note this is NOT lemmatizing using grammar pos\n",
        "\n",
        "def tokenize(text, modulation):\n",
        "    stop_words.add('MODERATOR')\n",
        "    if modulation<2:\n",
        "        tokens = re.split(r'\\W+', text)\n",
        "        stems = []\n",
        "        # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
        "        for token in tokens:\n",
        "            lowers=abbr_or_lower(token)\n",
        "            if lowers not in stop_words:\n",
        "                if re.search('[a-zA-Z]', lowers):\n",
        "                    if modulation==0:\n",
        "                        stems.append(lowers)\n",
        "                    if modulation==1:\n",
        "                        stems.append(porter.stem(lowers))\n",
        "    else:\n",
        "        sp_text=sp(text)\n",
        "        stems = []\n",
        "        lemmatized_text=[]\n",
        "        for word in sp_text:\n",
        "            lemmatized_text.append(word.lemma_)\n",
        "        stems = [abbr_or_lower(strip(w)) for w in lemmatized_text if (abbr_or_lower(strip(w))) and (abbr_or_lower(strip(w)) not in stop_words)]\n",
        "    return \" \".join(stems)\n",
        "\n",
        "\n",
        "def txtprocess_tok(corpus,col,mod):\n",
        "    text_preproc = (\n",
        "    corpus[col]\n",
        "    .astype(str)\n",
        "    .progress_apply(lambda row: tokenize(row, mod))\n",
        "    )\n",
        "    return text_preproc\n",
        "\n",
        "\n",
        "def vectorize(tokens, vocab):\n",
        "    vector=[]\n",
        "    for w in vocab:\n",
        "        vector.append(tokens.count(w))\n",
        "    return vector\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isy4Wby6Ol7k"
      },
      "source": [
        "## Part 1 - Fine tuning BERT models\n",
        "\n",
        "In this part, we will create a baseline model for text classification with BERT. This involves:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3CfPXOtPJ_5"
      },
      "source": [
        "### 1. **Loading and Exploring Data**:\n",
        "\n",
        "We will load the AG News corpus and perform necessary preprocessing steps like exploring the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zi8XOSH7PJGk",
        "outputId": "dba56546-3798-47eb-f993-1c428b6fa0a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.37.2\n",
            "  Downloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (4.66.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, transformers, datasets\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.38.2\n",
            "    Uninstalling transformers-4.38.2:\n",
            "      Successfully uninstalled transformers-4.38.2\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 transformers-4.37.2 xxhash-3.4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "b05bb6b5e0be4755904f38ee5915acde"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install datasets transformers==4.37.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "Nro-bNZcPOP4",
        "outputId": "d6ee7a72-a7b3-4df5-8793-3ce27651f29d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'datasets'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d35012874566>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the 'ag_news' dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rotten_tomatoes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the 'ag_news' dataset\n",
        "df = load_dataset(\"rotten_tomatoes\")\n",
        "\n",
        "# Explore the structure of the dataset\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFkLs38MPU3x"
      },
      "source": [
        "Defining Train, Validation and Test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAPU1X3IPVsN"
      },
      "outputs": [],
      "source": [
        "## Defining train, validation and test sets\n",
        "train_data = df['train']['text']\n",
        "train_labels = df['train']['label']\n",
        "\n",
        "validation_data = df['validation']['text']\n",
        "validation_labels = df['validation']['label']\n",
        "\n",
        "test_data = df['test']['text']\n",
        "test_labels = df['test']['label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jKu4kokPheE"
      },
      "source": [
        "### **Training DistilBert Model**:\n",
        "\n",
        "BERT-like Model Training (3 points)\n",
        "\n",
        "- Objective: Train a BERT-like model (BERT, DistilBERT, xtremedistill, etc.) on the Rotten Tomatoes dataset with a softmax layer on top of embeddings.\n",
        "\n",
        "- Tasks:\n",
        "1. Train the Model (1 point): Train your chosen model on the dataset and calculate accuracy, precision, recall, and F1-score.\n",
        "2. Error Analysis (1 point): Analyze false positives and false negatives to identify potential improvements.\n",
        "3. Feature Engineering (1 point): Implement your proposed improvements based on error analysis and show the impact on model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "iKBVaRoEPe7P",
        "outputId": "ec1e7457-7178-49be-f329-23ae0969952e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'transformers' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ef555ef3391f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTFAutoModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"distilbert-base-uncased\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'transformers' is not defined"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "transformers.__version__\n",
        "\n",
        "checkpoint= \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = TFAutoModel.from_pretrained(checkpoint)\n",
        "\n",
        "batch_size=64\n",
        "max_length=64\n",
        "rate = 0.5\n",
        "num_labels = len(np.unique(validation_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7K6BjfBQ2C0"
      },
      "source": [
        "The architecture is:\n",
        "1. Inputs ids and Inputs Masks\n",
        "2. Embedding layer --> the BERT model to process the inputs\n",
        "3. A layer upon the embeddings layer to convert the final [CLS] token into probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqkWKAy8T9wN",
        "outputId": "57573b2e-f5f6-455e-e93f-15fa64881148"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_token (InputLayer)    [(None, 64)]                 0         []                            \n",
            "                                                                                                  \n",
            " masked_token (InputLayer)   [(None, 64)]                 0         []                            \n",
            "                                                                                                  \n",
            " tf_distil_bert_model (TFDi  TFBaseModelOutput(last_hid   6636288   ['input_token[0][0]',         \n",
            " stilBertModel)              den_state=(None, 64, 768),   0          'masked_token[0][0]']        \n",
            "                              hidden_states=None, atten                                           \n",
            "                             tions=None)                                                          \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (  (None, 768)                  0         ['tf_distil_bert_model[0][0]']\n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " do_layer (Dropout)          (None, 768)                  0         ['tf.__operators__.getitem[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 2)                    1538      ['do_layer[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 66364418 (253.16 MB)\n",
            "Trainable params: 66364418 (253.16 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "## Input\n",
        "input_ids_in = tf.keras.layers.Input(shape=(max_length,), name='input_token', dtype='int32')\n",
        "input_masks_in = tf.keras.layers.Input(shape=(max_length,), name='masked_token', dtype='int32')\n",
        "\n",
        "# Embedding layers\n",
        "embedding_layer = model(input_ids=input_ids_in, attention_mask=input_masks_in)\n",
        "# we need only the first token representation nothing else from the last layer!\n",
        "final_embedding_layer = embedding_layer[0]#last layer of BERT\n",
        "# Extract only the [CLS] token's embeddings\n",
        "cls_token_embeddings = final_embedding_layer[:, 0, :]\n",
        "\n",
        "output_layer = tf.keras.layers.Dropout(rate, name='do_layer')(cls_token_embeddings)\n",
        "\n",
        "# One dense layer to process the last layer\n",
        "output = tf.keras.layers.Dense(num_labels,\n",
        "                               kernel_initializer=\"glorot_uniform\",\n",
        "                               activation='softmax')(output_layer)\n",
        "\n",
        "\n",
        "bert_model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = output)\n",
        "\n",
        "bert_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2mY3FnNWGVc"
      },
      "source": [
        "Let's generate some batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_pyH0r0XXqZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "def get_batches(X_train, y_train, tokenizer, batch_size, max_length):\n",
        "    \"\"\"\n",
        "    Objective: Create a generator that yields batches of tokenized text and corresponding labels.\n",
        "               The data is shuffled and looped through indefinitely.\n",
        "\n",
        "    Inputs:\n",
        "        - X_train (np.array): Array of text data (features).\n",
        "        - y_train (np.array): Array of labels.\n",
        "        - tokenizer (DistilBertTokenizer): Tokenizer for text data.\n",
        "        - batch_size (int): Size of each batch.\n",
        "        - max_length (int): Maximum length of tokenized sequences.\n",
        "    Outputs:\n",
        "        - Generator yielding batches of (inputs, targets).\n",
        "    \"\"\"\n",
        "\n",
        "    # Pre-tokenize the entire dataset\n",
        "    inputs = tokenizer.batch_encode_plus(list(X_train), add_special_tokens=True, max_length=max_length,\n",
        "                                         padding='max_length', return_attention_mask=True,\n",
        "                                         return_token_type_ids=True, truncation=True,\n",
        "                                         return_tensors=\"np\")\n",
        "\n",
        "    input_ids = np.asarray(inputs['input_ids'], dtype='int32')\n",
        "    attention_masks = np.asarray(inputs['attention_mask'], dtype='int32')\n",
        "\n",
        "    # Shuffle and yield batches\n",
        "    while True:\n",
        "        X_train, y_train, input_ids, attention_masks = shuffle(X_train, y_train, input_ids, attention_masks, random_state=11)\n",
        "\n",
        "        for i in range(0, len(X_train), batch_size):\n",
        "            yield [input_ids[i:i + batch_size], attention_masks[i:i + batch_size]], y_train[i:i + batch_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Maq8ugvo2HWo"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "y_train = enc.fit_transform(np.array(train_labels).reshape(-1, 1)).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sonuKBx70Mx0",
        "outputId": "f0216483-037c-401a-f959-42a5c93e281f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "133/133 [==============================] - 79s 370ms/step - loss: 0.4886 - recall: 0.7539 - precision: 0.7539\n",
            "Epoch 2/10\n",
            "133/133 [==============================] - 51s 382ms/step - loss: 0.3049 - recall: 0.8737 - precision: 0.8737\n",
            "Epoch 3/10\n",
            "133/133 [==============================] - 52s 389ms/step - loss: 0.1942 - recall: 0.9233 - precision: 0.9233\n",
            "Epoch 4/10\n",
            "133/133 [==============================] - 52s 389ms/step - loss: 0.1081 - recall: 0.9609 - precision: 0.9609\n",
            "Epoch 5/10\n",
            "133/133 [==============================] - 52s 389ms/step - loss: 0.0615 - recall: 0.9777 - precision: 0.9777\n",
            "Epoch 6/10\n",
            "133/133 [==============================] - 52s 388ms/step - loss: 0.0331 - recall: 0.9884 - precision: 0.9884\n",
            "Epoch 7/10\n",
            "133/133 [==============================] - 52s 389ms/step - loss: 0.0207 - recall: 0.9926 - precision: 0.9926\n",
            "Epoch 8/10\n",
            "133/133 [==============================] - 52s 389ms/step - loss: 0.0247 - recall: 0.9913 - precision: 0.9913\n",
            "Epoch 9/10\n",
            "133/133 [==============================] - 52s 389ms/step - loss: 0.0148 - recall: 0.9953 - precision: 0.9953\n",
            "Epoch 10/10\n",
            "133/133 [==============================] - 52s 389ms/step - loss: 0.0201 - recall: 0.9934 - precision: 0.9934\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79ee302bed40>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "X_train = np.array(train_data)\n",
        "\n",
        "steps_per_epoch = int(len(X_train) / batch_size)\n",
        "\n",
        "batches = get_batches(X_train, y_train, tokenizer, batch_size, max_length)\n",
        "\n",
        "bert_model.compile(optimizer=Adam(2e-5),\n",
        "                   metrics=[tf.keras.metrics.Recall(),\n",
        "                   tf.keras.metrics.Precision()],\n",
        "                   loss=tf.keras.losses.CategoricalCrossentropy())\n",
        "\n",
        "bert_model.fit(batches, epochs=10, steps_per_epoch=steps_per_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKAyX0hYAlEO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "outputId": "9e6666d0-d94b-4b6b-c556-433deb44479c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34/34 [==============================] - 4s 70ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1000 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Get the inputs tokenized from the validation set\n",
        "inputs = tokenizer.batch_encode_plus(list(validation_data),\n",
        "                                     add_special_tokens=True, max_length=max_length,\n",
        "                                    padding='max_length',  return_attention_mask=True,\n",
        "                                    return_token_type_ids=True, truncation=True)\n",
        "\n",
        "# Extract input_ids and attention mask\n",
        "inputs_valid = [np.asarray(inputs['input_ids'], dtype='int32'),\n",
        "               np.asarray(inputs['attention_mask'], dtype='int32')]\n",
        "valid_preds = bert_model.predict(inputs_valid)\n",
        "valid_preds = np.argmax(valid_preds, axis=1)\n",
        "\n",
        "# get the final metrics\n",
        "metrics_val= Metrics()\n",
        "metrics_val.run(validation_labels, valid_preds, \"distilBERT + softmax\")\n",
        "metrics_val.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDGcWm48IG3F"
      },
      "source": [
        "Error Analysis (1 point): Analyze false positives and false negatives to identify potential improvements."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming cm2 is your confusion matrix\n",
        "cm = confusion_matrix(validation_labels, valid_preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['negative', 'positive'])\n",
        "disp.plot(cmap='Blues')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "32K5ORz7MZ5j",
        "outputId": "5bf3ca0c-07ce-4f17-c2a1-7e08df0a9503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'validation_labels' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d567da1d0cff>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Assuming cm2 is your confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdisp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfusionMatrixDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'negative'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'positive'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdisp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Blues'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'validation_labels' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Error analysis"
      ],
      "metadata": {
        "id": "hqLp0ChUZJ4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Error Analysis - Evaluate text wrongly classified in validation data\n",
        "import pandas as pd\n",
        "\n",
        "valid_df = pd.DataFrame({'review':validation_data,'label':validation_labels,'pred':valid_preds})\n",
        "\n",
        "#Return only text where pred is different from label\n",
        "valid_df_diff = valid_df[valid_df['label']!= valid_df['pred']]\n",
        "\n",
        "#Reduce dimensionality\n",
        "print(valid_df_diff.shape)\n",
        "valid_df_diff.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "X4YZTTcAYBvN",
        "outputId": "0dfd0400-ef1b-439a-fce5-bdfeb0937cf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(179, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               review  label  pred\n",
              "14  the importance of being earnest , so thick wit...      1     0\n",
              "16  made for teens and reviewed as such , this is ...      1     0\n",
              "17  imagine o . henry's <b>the gift of the magi</b...      1     0\n",
              "22  nothing short of wonderful with its ten-year-o...      1     0\n",
              "38  broomfield reminds us that beneath the hype , ...      1     0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-add9d84c-a288-4b3e-8fba-d758ff45f749\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>the importance of being earnest , so thick wit...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>made for teens and reviewed as such , this is ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>imagine o . henry's &lt;b&gt;the gift of the magi&lt;/b...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>nothing short of wonderful with its ten-year-o...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>broomfield reminds us that beneath the hype , ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-add9d84c-a288-4b3e-8fba-d758ff45f749')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-add9d84c-a288-4b3e-8fba-d758ff45f749 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-add9d84c-a288-4b3e-8fba-d758ff45f749');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9ccb3aff-9fde-4730-a569-9acae1b33f9b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9ccb3aff-9fde-4730-a569-9acae1b33f9b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9ccb3aff-9fde-4730-a569-9acae1b33f9b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "valid_df_diff",
              "summary": "{\n  \"name\": \"valid_df_diff\",\n  \"rows\": 179,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 179,\n        \"samples\": [\n          \"it may be a no-brainer , but at least it's a funny no-brainer .\",\n          \"not only is it a charming , funny and beautifully crafted import , it uses very little dialogue , making it relatively effortless to read and follow the action at the same time .\",\n          \"after one gets the feeling that the typical hollywood disregard for historical truth and realism is at work here , it's a matter of finding entertainment in the experiences of zishe and the fiery presence of hanussen .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Text mining - Preprocessing, CountVectorizing and tf-idf\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "##Preprocessing Valdidation_diff text\n",
        "valid_df_diff_pre = txtprocess_tok(valid_df_diff,'review',2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iI69sOU0ZZfM",
        "outputId": "c5b8b9b3-aa91-4a8e-97f6-2ee874717b03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 179/179 [00:03<00:00, 44.80it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Applying Pipeline\n",
        "pipeline = Pipeline(\n",
        "    [\n",
        "        (\"vect\", CountVectorizer(ngram_range = (1,3),  min_df=0.05,max_df=0.9, stop_words='english')),\n",
        "        (\"tfidf\", TfidfTransformer(use_idf=True, norm='l1')),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Fit the pipeline to the validation data\n",
        "pipeline.fit(validation_texts)\n",
        "\n",
        "# Transform the validation data using the fitted pipeline\n",
        "validation_tfidf = pipeline.transform(validation_texts)\n"
      ],
      "metadata": {
        "id": "qEkpnjvEgLCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FyX3KKqcFKo"
      },
      "source": [
        "Let's try to freeze the BERT models to see the differences and the knowlegde of BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnWzxp0JcLlD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "## Input\n",
        "input_ids_in =\n",
        "input_masks_in =\n",
        "\n",
        "# Embedding layers\n",
        "# we need only the first token representation nothing else !\n",
        "embedding_layer =\n",
        "# Let's add some dropout to reduce overfitting\n",
        "output_layer =\n",
        "\n",
        "# One dense layer to process the last layer\n",
        "output =\n",
        "\n",
        "\n",
        "bert_model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = output)\n",
        "bert_model.layers[2].trainable = False\n",
        "bert_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hs_gq4l3cneu"
      },
      "outputs": [],
      "source": [
        "batches = get_batches(X_train, y_train, tokenizer, batch_size, max_length)\n",
        "#Compile the model with gradient descent algorithm and metrics\n",
        "bert_model.compile(\n",
        "\n",
        ")\n",
        "#fit the model\n",
        "bert_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haTc_2XNcnyG"
      },
      "outputs": [],
      "source": [
        "valid_preds =\n",
        "valid_preds =\n",
        "\n",
        "metrics_val.run(valid_labels, valid_preds, \"distilBERT frozen + Softmax\")\n",
        "metrics_val.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfUla6fKAe2O"
      },
      "source": [
        "## Part 2: Few Shot Learning\n",
        "\n",
        "In this part, we'll explore few shot learning with SetFit. We will train the model with low number of examples and try to augment the dataset with prompts !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KmEy9JJxFCW"
      },
      "source": [
        "### Loading SetFit\n",
        "\n",
        "Let's begin by setting-up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tYKtZUTDDJw"
      },
      "outputs": [],
      "source": [
        "!pip install setfit\n",
        "\n",
        "from setfit import SetFitModel, Trainer, TrainingArguments, sample_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NCk1cgdjE5t"
      },
      "source": [
        "### Training the SetFit with 32 examples\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNaWjFq0lqgo"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Convert data into Dataset object from datasets\n",
        "train_dict = {'text': train_data, 'label': train_labels}\n",
        "\n",
        "train_dataset = Dataset.from_dict(train_dict)\n",
        "\n",
        "dataset_dict = {'train': train_dataset}\n",
        "\n",
        "#let's sample 32 examples at first to see results\n",
        "train_dataset = sample_dataset(\n",
        "\n",
        "                               )\n",
        "\n",
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwhD_YQ4mtkp"
      },
      "outputs": [],
      "source": [
        "#Model to load\n",
        "model = SetFitModel.from_pretrained(\n",
        "\n",
        ")\n",
        "\n",
        "#Arguments / hyperparamters to train\n",
        "args = TrainingArguments(\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "# Trainer class to train afterwards\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset_augmented,\n",
        "    metric=\"accuracy\",\n",
        ")\n",
        "\n",
        "# Train and evaluate\n",
        "trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lNWofcToplk"
      },
      "outputs": [],
      "source": [
        "valid_preds =\n",
        "metrics_val.run(valid_labels, valid_preds, \"SetFit 32 examples\")\n",
        "metrics_val.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmEwwnvjjJzw"
      },
      "source": [
        "### Try to augment the data with prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pn1_H6IskExm"
      },
      "outputs": [],
      "source": [
        "labels = {0: 'World', 1: 'Sports',\n",
        "          2: 'Business', 3: 'Sci/Tech'}\n",
        "labels_to_id = {value:key for key, value in labels.items()}\n",
        "\n",
        "\n",
        "from transformers import pipeline\n",
        "# Load zero-shot classification pipeline\n",
        "classifier =\n",
        "\n",
        "# Define the sequence to classify\n",
        "texts =\n",
        "\n",
        "# Define the candidate labels\n",
        "candidate_labels =\n",
        "\n",
        "# Perform zero-shot classification\n",
        "results ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BN4zuwDPrGGp"
      },
      "outputs": [],
      "source": [
        "#select texts\n",
        "texts =\n",
        "#apply classifier\n",
        "results ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4Aut70ut9-f"
      },
      "outputs": [],
      "source": [
        "labels_to_id = {value:key for key, value in labels.items()}\n",
        "\n",
        "new_texts = []\n",
        "new_labels = []\n",
        "th = 0.6\n",
        "\n",
        "for text, result in zip(texts, results):\n",
        "  if max(result.get('scores')) < th:\n",
        "    continue\n",
        "\n",
        "  new_texts.append(text)\n",
        "  new_labels.append(labels_to_id.get(result.get('labels')[0]))\n",
        "\n",
        "np.unique(new_labels, return_counts=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nledOU1ls0W4"
      },
      "outputs": [],
      "source": [
        "train_data_augmented = train_dataset['text'] + new_texts\n",
        "train_labels_augmented = train_dataset['label'] + new_labels\n",
        "\n",
        "train_dict_augmented = {'text': train_data_augmented, 'label': train_labels_augmented}\n",
        "\n",
        "train_dataset_augmented = Dataset.from_dict(train_dict_augmented)\n",
        "\n",
        "train_dataset_augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojwxqeo1w0mU"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Model to load\n",
        "model = SetFitModel.from_pretrained(\n",
        "\n",
        ")\n",
        "\n",
        "#Arguments / hyperparamters to train\n",
        "args = TrainingArguments(\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "# Trainer class to train afterwards\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset_augmented,\n",
        "    metric=\"accuracy\",\n",
        ")\n",
        "\n",
        "# Train and evaluate\n",
        "trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJukVE-DjJ-G"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFTYoptExhj9"
      },
      "outputs": [],
      "source": [
        "#Makes some preds\n",
        "valid_preds =\n",
        "metrics_val.run(valid_labels, valid_preds, \"SetFit augmented\")\n",
        "metrics_val.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Em7rcoLxzux"
      },
      "source": [
        "## Part 3: Winogender Schemas\n",
        "\n",
        "In this work, we will explore biases of the BERT model. We will begin with the [Winograd Schemas adapted](https://en.wikipedia.org/wiki/Winograd_schema_challenge) to gender: [Winogender schemas](https://github.com/rudinger/winogender-schemas#winogender-schemas). This dataset is extracted from [Rudinger et al. (2018)](https://aclanthology.org/N18-2002/).\n",
        "\n",
        "\n",
        "From Wikipedia:\n",
        "\n",
        "> The Winograd schema challenge (WSC) is a test of machine intelligence proposed by Hector Levesque, a computer scientist at the University of Toronto. Designed to be an improvement on the Turing test, it is a multiple-choice test that employs questions of a very specific structure: they are instances of what are called Winograd schemas. Questions of this form may be tailored to require knowledge and commonsense reasoning in a variety of domains.\n",
        ">\n",
        "> A Winograd schema challenge question consists of three parts:\n",
        "> 1. A sentence or brief discourse that contains the following:\n",
        "> *   Two noun phrases of the same semantic class (male, female, inanimate or group of objects or people),\n",
        "> *   An ambiguous pronoun that may refer to either of the above noun phrases, and\n",
        "> *   A special word and alternate word, such that if the special word is replaced with the alternate word, the natural resolution of the pronoun changes.\n",
        "> 2. A question asking the identity of the ambiguous pronoun, and\n",
        "> 3. Two answer choices corresponding to the noun phrases in question.\n",
        ">\n",
        "> A machine will be given the problem in a standardized form which includes the answer choices, thus making it a binary decision problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjRVdiPHjP9N"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/rudinger/winogender-schemas/master/data/templates.tsv'\n",
        "df = pd.read_csv(url, sep='\\t')\n",
        "\n",
        "df.loc[:, 'whole_sentence'] = df.loc[:, ['sentence',\n",
        "                                         'occupation(0)',\n",
        "                                         'other-participant(1)']].apply(lambda x:\n",
        "                                         x[0].replace('$OCCUPATION', x[1]).replace('$PARTICIPANT', x[2]), axis=1)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTHZzTk_y_63"
      },
      "source": [
        "### Gender bias identification\n",
        "\n",
        "In this part, we will take a look at ungendered occupation / participant, and see what are the main distribution of gender pronouns.\n",
        "\n",
        "We will use simp,ly the pipeline to fill mask from huggingface that is really efficient. We can do it for several different models in order to see what are the differences between models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ket5COBzR4n"
      },
      "source": [
        "#### BERT-large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKRTR3zJzBhI"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "checkpoint=\"bert-large-uncased\"\n",
        "classifier = pipeline('fill-mask', model=checkpoint)\n",
        "\n",
        "regex = '\\$(\\w+)'\n",
        "df.loc[:, 'pronoun'] = df.loc[:, 'whole_sentence'].apply(lambda x: re.findall(regex, x)[0])\n",
        "words_to_replace = ['\\$' + x for x in df.loc[:, 'pronoun'].unique()]\n",
        "regex = r'(?:{})'.format('|'.join(words_to_replace))\n",
        "df.loc[:, 'sentence_mask'] = df.loc[:, 'whole_sentence'].apply(lambda x: re.sub(regex, '[MASK]', x))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ni0nA4PZzcbg"
      },
      "outputs": [],
      "source": [
        "pronouns = {'ACC_PRONOUN': ['her', 'him'],\n",
        "            'NOM_PRONOUN': ['she', 'he'],\n",
        "            'POSS_PRONOUN': ['her', 'his']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YakSnxGHzeRo"
      },
      "outputs": [],
      "source": [
        "res = {}\n",
        "for key, value in pronouns.items():\n",
        "  res[key] = []\n",
        "  texts = list(df.loc[df.loc[:, 'pronoun'] == key, 'sentence_mask'].values)\n",
        "  res[key] += classifier(texts, targets=value, top_k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJJbQQ0FzgIl"
      },
      "outputs": [],
      "source": [
        "probas = {}\n",
        "for key, value in pronouns.items():\n",
        "  probas[key] = [x[0]['score'] / (x[0]['score'] + x[1]['score'])\n",
        "                 if x[0]['token_str'] == value[0]\n",
        "                 else x[1]['score'] / (x[0]['score'] + x[1]['score'])\n",
        "    for x in res[key]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87lkg5u_zhui"
      },
      "outputs": [],
      "source": [
        "for key, value in pronouns.items():\n",
        "  df.loc[df.loc[:, 'pronoun'] == key, 'BERT-large-female'] = probas[key]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg9zISFGzmnd"
      },
      "source": [
        "#### BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_kzpEbszn0n"
      },
      "outputs": [],
      "source": [
        "checkpoint=\"bert-base-uncased\"\n",
        "classifier = pipeline('fill-mask', model=checkpoint)\n",
        "\n",
        "res = {}\n",
        "probas = {}\n",
        "for key, value in pronouns.items():\n",
        "  res[key] = []\n",
        "  texts = list(df.loc[df.loc[:, 'pronoun'] == key, 'sentence_mask'].values)\n",
        "  res[key] += classifier(texts, targets=value, top_k=2)\n",
        "  probas[key] = [x[0]['score'] / (x[0]['score'] + x[1]['score'])\n",
        "                 if x[0]['token_str'] == value[0]\n",
        "                 else x[1]['score'] / (x[0]['score'] + x[1]['score'])\n",
        "    for x in res[key]]\n",
        "  df.loc[df.loc[:, 'pronoun'] == key, 'BERT-female'] = probas[key]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yRHHQtUzrvR"
      },
      "source": [
        "### distillBERT\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLNeYcf_zwXn"
      },
      "outputs": [],
      "source": [
        "checkpoint=\"distilbert-base-uncased\"\n",
        "classifier = pipeline('fill-mask', model=checkpoint)\n",
        "\n",
        "res = {}\n",
        "probas = {}\n",
        "for key, value in pronouns.items():\n",
        "  res[key] = []\n",
        "  texts = list(df.loc[df.loc[:, 'pronoun'] == key, 'sentence_mask'].values)\n",
        "  res[key] += classifier(texts, targets=value, top_k=2)\n",
        "  probas[key] = [x[0]['score'] / (x[0]['score'] + x[1]['score'])\n",
        "                 if x[0]['token_str'] == value[0]\n",
        "                 else x[1]['score'] / (x[0]['score'] + x[1]['score'])\n",
        "    for x in res[key]]\n",
        "  df.loc[df.loc[:, 'pronoun'] == key, 'distilBERT-female'] = probas[key]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIzR_rU2z253"
      },
      "source": [
        "#### Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zMLeMA4z1YM"
      },
      "outputs": [],
      "source": [
        "probas = {}\n",
        "\n",
        "for occ, part, answer, l, b, d in df.loc[:, ['occupation(0)', 'other-participant(1)', 'answer',\n",
        "                                             'BERT-large-female', 'BERT-female', 'distilBERT-female']].values:\n",
        "  p = occ if answer == 0 else part\n",
        "  if p in probas.keys():\n",
        "    probas[p].append([l, b, d])\n",
        "  else:\n",
        "    probas[p] = [[l, b, d]]\n",
        "\n",
        "for key, value in probas.items():\n",
        "  probas[key] = np.mean(value, axis=0)\n",
        "\n",
        "probas = sorted(probas.items(), key=lambda x: x[1][0], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgbG_Y-hz9sx"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(30, 8))\n",
        "\n",
        "ax.scatter(np.arange(len(probas)), [x[1][0] for x in probas], label='BERT-large')\n",
        "ax.scatter(np.arange(len(probas)), [x[1][1] for x in probas], label='BERT')\n",
        "ax.scatter(np.arange(len(probas)), [x[1][2] for x in probas], label='distilBERT')\n",
        "\n",
        "ax.hlines(0.5, 0, len(probas), colors='g')\n",
        "ax.set_xticks(np.arange(len(probas)))\n",
        "ax.set_xticklabels([x[0] for x in probas], rotation=90)\n",
        "ax.set_xlabel(\"occupations\")\n",
        "ax.set_ylabel(\"probability to be women\")\n",
        "ax.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd90nubF0VnT"
      },
      "outputs": [],
      "source": [
        "CHANGESSSSSS CHANGESSSS"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}